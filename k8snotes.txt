# k8s provides the following:
  - Declarative Deployment
  - Resiliency and Self-healing
  - Horizontal Scaling
  - Rolling Updates
  - Resource Management
  - Load Balancing
  - Storage Orchestration
  - Configuration Management
  - Vendor Neutrality
  - Extensibility
  - Service Discovery & Networking
  - Liveness & Readiness


# PODS AND CONTAINERS

Kubernetes is a container orchestration platform that coordinates the collaboration of Master Nodes and Worker Nodes. Master Nodes (Control Plane) are responsible for scheduling and deciding where applications run. Worker Nodes provide the infrastructure to actually run the applications. In a single-node cluster, your computer plays the role of Master and Worker Node.

Containers
Containers run applications in isolation with their dependencies, making them highly portable.

Pods
In Kubernetes, pods are the smallest deployable units and encapsulate application containers.

Pod Configuration

Metadata:
Name: Uniquely identifies the pod
Labels: Categorize pods into distinct groups for flexible querying
Runtime Requirements (specified under 'spec'):
Container name
Image sourcE
Port for serving requests
Resource requirements (CPU and memory)
Memory limit and request should be the same
CPU limit should rarely be set (as per Kubernetes best practices)

Multi-Container Pods
Pods can run multiple containers, enabling sidecar patterns where auxiliary sidecar containers can communicate with the main application container via localhost.

Port Forwarding in Kubernetes
Port forwarding creates a temporary connection between your local machine and a pod in the cluster. It's primarily used for debugging and testing purposes. The command structure is:  kubectl port-forward <pod-name> <local-port>:<pod-port>. For example, kubectl port-forward mypod 808


# SERVICE DISCOVERY

The Service primitive in Kubernetes abstracts away network complexities and provides a durable endpoint for accessing pods.

NodePort Service
Allows external access to the Kubernetes network by exposing a static port on the node (range: 30000-32767).

apiVersion: v1
kind: Service
metadata:
  name: grade-submission-portal
spec:
  type: NodePort
  selector:
    app: grade-submission-portal
  ports:
    - port: 8080
      targetPort: 8080
      nodePort: 30080
      
The external request is initiated on the node's static port  (nodePort: 30080).

The request enters the cluster through the Service's internal port (port: 8080). With NodePort services, this internal port can be any valid port number, as the external nodePort (30080) will be mapped to whatever internal port is specified.

The service acts as a proxy by directing the request to a matching pod using a label selector.

The service specifies a target port (targetPort: 8080) that ensures the request reaches the container port on the pod.

Note: The NodePort service is often used when prototyping, rarely in practice.

ClusterIP Service
Used for internal pod-to-pod communication within the cluster.

apiVersion: v1
kind: Service
metadata:
  name: backend-service
spec:
  type: ClusterIP
  selector:
    app: backend
  ports:
    - port: 8080
      targetPort: 8080

Pods within the cluster can access the service using its name (backend-service) and service's internal port (port: 8080).

The service acts as a proxy by directing the request to a matching pod using a label selector.

The service specifies a target port (targetPort: 8080) that ensures the request reaches the container port on the pod.


# NAMESPACES

While a Kubernetes cluster is physically distributed across multiple machines (master and worker nodes), developers interact with it as a single entity partitioned into logical divisions called namespaces. These namespaces group together closely related resources.

Deploying to a Specific Namespace
Command-line approach: kubectl apply -f my-deployment.yaml -n my-namespace

In YAML configuration:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
  namespace: my-namespace


Default Namespace
If no namespace is specified, Kubernetes will place the resource in the default namespace.

Best Practices for Developers
Use namespaces to organize and isolate your workloads logically.

Be aware of the namespace you're working in to avoid unintended interactions between resources.

Collaborate with your cluster administrators to understand any namespace-level policies or quotas that may affect your deployments.


# SELF-HEALING AND RESILIENCY

By default, every container in a pod has a restart policy of "Always". This means Kubernetes restarts a container whenever its process terminates, regardless of the exit code.

Behind the Scenes
Kubernetes Controllers: Various controllers work tirelessly in the background to maintain the desired state of your applications.

Desired State: The controllers compare the current state of the pod with its desired state and take actions to reconcile any differences.

Continuous Monitoring: Kubernetes constantly monitors the health of containers and takes corrective actions when needed.

Process Termination vs. Application Unresponsiveness
In the previous lesson, we saw how Kubernetes automatically detects and restarts terminated containers. Later in this course, we'll cover liveness and readiness probes, which allow you to configure Kubernetes to detect unresponsive applications and restart them automatically.

To prepare for this, it's important to understand the difference between process termination and application unresponsiveness:

Process Termination:

Kubernetes automatically detects when a process within a container terminates.

Kubernetes immediately takes action based on the restart policy, without any additional configuration, as seen in the lesson.

Examples:

Out of Memory (OOM) Error: If a container exceeds its memory limit, the process is terminated by the system, triggering a restart.

Application Crash: If your application encounters an unhandled exception and exits, Kubernetes will restart the container.

Application Unresponsiveness:

This occurs when an application is still running but not functioning correctly.

Kubernetes cannot automatically detect this situation without us additionally configuring liveness probes, which will be covered in a future lesson.

Examples:

Deadlock Situations: In cases where your application becomes unresponsive due to a deadlock, Kubernetes can restart the container if configured with appropriate liveness probes (a topic for future lessons).


# DEPLOYMENT AND POD REPLICAS

A Deployment in Kubernetes allows us to declaratively manage a set of pods. The two key specifications in a Deployment are the number of pod replicas, and the pod template (defining how each pod should be constructed). This Deployment declares our desired state: 3 replicas of pods based on the specified template:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: my-app
        image: my-app:v1
Behind the scenes:

The Deployment Controller creates a ReplicaSet based on this specification.

A ReplicaSet is a Kubernetes resource that ensures a specified number of pod replicas are running at all times.

The ReplicaSet Controller watches for ReplicaSet objects and creates pods based on them.

The ReplicaSet Controller continuously monitors the state of the pods.

If a pod terminates or fails, the ReplicaSet Controller automatically creates a new one to maintain the desired number of replicas.

The Deployment Controller manages the overall lifecycle of the Deployment, including updates and rollbacks (discussed in the next section), while the ReplicaSet Controller handles the day-to-day management of pods. This orchestration happens automatically. As developers, we simply declare our desired state in the Deployment object, and Kubernetes handles the rest, ensuring our application remains running as specified.

# ROLLING UPDATE

Without rolling updates, upgrading an application could lead to significant downtime. Imagine a scenario where all pod replicas running v1 of an application are simultaneously taken down and replaced with v2. This would result in a period where no pods are available to serve traffic, causing service interruption. Rolling updates solve this problem by gradually replacing old pods with new ones, ensuring that the application remains available throughout the upgrade process.

How Rolling Updates Work
1.  The process begins when you update the pod template in a Deployment resource. Here's an example:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: grade-submission-api
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: grade-submission-api
        image: registry/grade-submission-api:v1 # Update to v2
2. The Deployment controller creates a new ReplicaSet for v2 pods while preserving the old ReplicaSet for v1 pods. A ReplicaSet is a Kubernetes resource that ensures a specified number of pod replicas are running at all times.

During a rolling update, the Deployment manages two ReplicaSets:

   ReplicaSet 1 (v1): 3 pods ----> 0 pods
   ReplicaSet 2 (v2): 0 pods ----> 3 pods
3. New v2 pods are created while old v1 pods are gradually terminated. This process continues until all pods are running the new version.

# Controlling the Rolling Update
By default, Kubernetes implements a rolling update strategy, but you can control its behavior using two parameters:

maxUnavailable: The maximum number of pods that can be unavailable during the update.

maxSurge: The maximum number of pods that can be created over the desired number of pods.

Here's how you can specify these in your Deployment:

spec:
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
With these settings and 3 replicas, you might see output like this during the update:

NAME                                   READY   STATUS
grade-submission-api-7f9dd7f789-2x9p4   1/1     Running
grade-submission-api-7f9dd7f789-9k2xn   1/1     Running
grade-submission-api-7f9dd7f789-qp6g7   1/1     Running
grade-submission-api-6c54bd5869-lmn0p   0/1     ContainerCreating
This output reflects our settings:

All three original pods are running, respecting maxUnavailable: 1.

One new pod is being created, adhering to maxSurge: 1.

As the update progresses:

NAME                                   READY   STATUS
grade-submission-api-7f9dd7f789-2x9p4   1/1     Running
grade-submission-api-7f9dd7f789-9k2xn   1/1     Running
grade-submission-api-7f9dd7f789-qp6g7   0/1     Terminating
grade-submission-api-6c54bd5869-lmn0p   1/1     Running
grade-submission-api-6c54bd5869-3fgh2   1/1     Running
Here, we see:

One old pod terminating (maxUnavailable: 1)

Two new pods running, temporarily giving us 4 pods total (maxSurge: 1)

At least 2 pods always fully running, ensuring minimal disruption

This process continues until all pods are updated to the new version.

Rollbacks
If issues arise with the new version, Kubernetes makes rollbacks easy. The old ReplicaSet (v1) is kept, allowing for quick reversion. During a rollback:

1. The v2 ReplicaSet is scaled down to 0 pods.

2. A new ReplicaSet based on the v1 configuration is created and scaled up to the desired number of replicas.

3. This process also uses a rolling update strategy, ensuring minimal downtime during the rollback.

ReplicaSet 1 (v1): 3 pods ----> 0 pods
ReplicaSet 2 (v2): 3 pods ----> 0 pods
ReplicaSet 3 (v1): 0 pods ----> 3 pods
Benefits of Rolling Updates
1. Minimal Downtime: By gradually replacing pods, the application remains available throughout the update process.

2. Version Control: Easy rollbacks to previous versions if issues are detected.

3. Traffic Management: Ensures that user traffic is always directed to available pods.


# LIVENESS AND READINESS PROBES

Liveness The liveness endpoint returns a 200 status if the application is operational, or a 500 status if it's not. The liveness probe checks this endpoint and considers the app healthy only if it receives a 200 status. Any other response, or no response at all, triggers a container restart.

Readiness The readiness endpoint verifies if the application has successfully connected to all components necessary for serving traffic. It returns a 200 status only when all required connections and initializations are complete, and a 500 status otherwise. The readiness probe uses this endpoint to determine if a container is ready to accept traffic. If the probe receives anything other than a 200 status, or no response, it keeps the container out of service.

Initial Delay and Period The initial delay sets how long to wait before the first probe runs, while the period determines the frequency of subsequent probes. For example, if an app takes 20 seconds to start, set the liveness probe's initial delay to at least 20 seconds. This ensures the probe only begins checking after the application has had sufficient time to initialize:

livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 25
  periodSeconds: 5


For readiness probes, the initial delay is less critical. An unresponsive app during startup correctly indicates it's not yet ready for traffic.

readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  periodSeconds: 5


Usage Use liveness probes to detect and restart unhealthy containers. Use readiness probes to determine when a container is ready to start accepting traffic. Together, they ensure your application remains healthy and responsive in a Kubernetes environment.


# STATEFULSET AND PERSISTENT VOLUMES

Anatomy of a Connection String
Understanding the structure of basic authentication, where credentials are included directly in the URI, will be valuable in our upcoming lessons. This URI is composed of three main parts: Scheme, Auth Credentials, and Service Location.


When a client (grade-submission-api) uses this URI to connect to MongoDB, here's what happens:

Kubernetes routing is based solely on the Service Location (mongodb-service:27017), regardless of the Scheme used (mongodb://, http://, https://, etc.), or the existence of credentials.

The ClusterIP Service mongodb-service receives the request at its service port 27017.

The Service selects one of the MongoDB pods based on a label selector, and forwards the entire original URI to the selected pod, preserving all parts (Scheme, Auth Credentials, and Service Location).

MongoDB receives the full URI at its container port 27017, and authenticates the connection using the provided credentials.

It's worth noting that this process is consistent across different types of services in Kubernetes. For example:

Elasticsearch

MySQL

In each case, Kubernetes focuses on the Service Location for routing, while the application (MySQL or Elasticsearch) handles the Scheme and Auth Credentials as needed.


Managing stateful applications and their storage requirements is time-consuming and error-prone.

Automatic Storage Orchestration
With Kubernetes, you can fully rely on the control plane to manage storage for your stateful applications:

Automatic provisioning and attachment of storage volumes

Seamless data persistence across pod lifecycle events

Built-in mechanisms for maintaining pod identity and state

Key Components of StatefulSet Storage
Persistent Volume Claims (PVCs): Requests for storage resources

Volume Claim Templates:

Define the PVC specification within the StatefulSet

Automatically generate PVCs for each pod in the StatefulSet

Example: If a StatefulSet has 3 replicas, 3 PVCs will be dynamically created

Persistent Volumes (PVs): Actual storage resources that fulfill PVCs

Storage Orchestration:

Each StatefulSet pod gets its own PVC, generated from the template

PVC binds to an appropriate PV

PV is mounted at the specified directory in the stateful container


# CONFIGMAP & SECRETS

ConfigMaps in Kubernetes are used to store non-confidential configuration data in key-value pairs, allowing you to decouple configuration from pod specifications and make your applications more portable. They can be consumed by pods as environment variables and updated without rebuilding your application container.

apiVersion: v1
kind: ConfigMap
metadata:
  name: game-config
data:
  player_initial_lives: "3"
  ui_properties_file_name: "user-interface.properties"


Secrets in Kubernetes are similar to ConfigMaps but are specifically designed for sensitive information like passwords, OAuth tokens, and SSH keys. They can be consumed by pods similarly to ConfigMaps. Secrets use the data field, which expects base64 encoded values. This encoding is particularly useful for handling special characters often found in sensitive data:

apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  username: YWRtaW4=  # base64 encoded 'admin'
  password: dDBwLVMzY3IzdA==  # base64 encoded 't0p-S3cr3t'


Important Note: while Secrets are base64 encoded, they are not encrypted. Additional security measures are typically implemented to protect sensitive data in clusters. These can include external secret management systems, or implementing Kubernetes Encryption Providers. However, specific approaches are beyond the scope of this overview as they vary based on organizational needs and security policies.


# HORIZONTAL POD SCALING (HPA)

Key Takeaways
The Horizontal Pod Autoscaler automatically scales the number of pods in a deployment based on observed CPU utilization (most commonly).

Key Components of an HPA Configuration
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: grade-submission-portal-hpa
  namespace: grade-submission
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: grade-submission-portal
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
        
Autoscaling Behavior: The HPA will increase or decrease the number of replicas to maintain the target CPU utilization.

Scaling Range: The number of pods will be adjusted between 1 and 10 based on the CPU utilization.

Resource Metrics: While this example uses CPU, HPAs can also use memory or custom metrics.

Target Utilization: 50% target utilization is a common starting point, but this can be adjusted based on application needs.

Namespace Scoping: The HPA is namespace-specific, allowing for isolated scaling policies across different parts of your application.

Scaling Algorithm: Kubernetes uses a control loop to periodically adjust the number of replicas based on the observed metrics.

Understanding and effectively configuring HPAs is crucial for building scalable and efficient applications in Kubernetes, ensuring optimal resource utilization and performance under varying loads.


# INGRESS CONTROLLER

The Ingress controller in Kubernetes acts as a reverse proxy, which means it sits in front of web servers and acts on their behalf to forward external HTTP requests to the appropriate internal services.

The Ingress controller uses the Ingress resource to determine how to route traffic. Here's a step-by-step breakdown of how it works:

  1.    An external HTTP request arrives at the Ingress controller.

  2.   The controller examines the Ingress resource, which is defined as follows:

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grade-submission-portal-ingress
  namespace: grade-submission
spec:
  ingressClassName: nginx
  rules:     
  - http:
      paths:
      - pathType: Prefix
        path: "/"    
        backend:
          service:
            name: grade-submission-portal
            port: 
              number: 5001
              
  3.   The controller identifies that this is an Ingress resource (kind: Ingress) in the grade-submission namespace.

  4.   It notes that the nginx Ingress controller should be used (ingressClassName: nginx).

  5.   The controller then looks at the rules. In this case, there's a single rule that applies to all HTTP traffic.

  6.   The rule specifies that all paths starting with / (path: "/") should be directed to the grade-submission-portal service on port 5001.

  7.    Based on this rule, the Ingress controller forwards the request to the specified backend service.

In the current configuration, the rules are quite permissive. Any host can connect, and all traffic is routed to the same service. This setup is common in development environments but may not be suitable for production.

In a production environment, after purchasing a domain, you can implement more restrictive rules. For example:

spec:
  rules:
  - host: grades.myuniversity.com
    http:
      paths:
      - path: "/"
        pathType: Prefix
        backend:
          service:
            name: grade-submission-portal
            port: 
              number: 5001

This configuration would only allow traffic from the specified host (grades.myuniversity.com) to be routed to the service. Any requests from other hosts would be rejected, providing an additional layer of security and control over incoming traffic.


## HELM CHART

Helm is a package manager for Kubernetes that simplifies the deployment and management of applications. By using Helm Charts, you can manage complex Kubernetes applications as a single unit, simplifying deployment, upgrades, and rollbacks. This approach provides a more organized and maintainable way to handle Kubernetes resources compared to managing individual loose resources.
Directory Structure

A typical Helm Chart has the following structure:

    mychart/
      ├── Chart.yaml          # Contains chart information
      ├── values.yaml         # Default configuration values
      └── templates/          # Directory for template files
          ├── deployment.yaml
          ├── service.yaml
          └── ingress.yaml

    Chart.yaml: Metadata about the chart (name, version, dependencies)

    values.yaml: Default configuration values that can be overridden

    templates/: Directory containing Kubernetes manifest templates

Common Helm Commands

    Install a chart:

        helm install [RELEASE_NAME] [CHART]

    Uninstall a release:

        helm uninstall [RELEASE_NAME]

    Upgrade a release:

        helm upgrade [RELEASE_NAME] [CHART]

    List all releases:

        helm list

    View the manifest templates with values applied:

        helm template [CHART]

    Package a chart into a versioned archive file:

        helm package [CHART_PATH]

Deployment Workflow

    Create or modify chart files in the chart directory.

    Use helm template to preview the rendered Kubernetes manifests.

    Install or upgrade the release using helm install or helm upgrade.

    Monitor the deployment using kubectl commands.

    If needed, use helm uninstall to remove the release and all associated resources.


## HELM PACKAGE MANAGER

Helm serves as a powerful package manager for Kubernetes, simplifying the deployment of complex software like Elasticsearch, MongoDB, MySQL, and Redis.
Process of Deploying Complex Software with Helm

    Add Helm Repository: helm repo add bitnami https://charts.bitnami.com/bitnami

    This adds the Bitnami repository, which hosts many popular software charts.

    Update Helm Repositories: helm repo update

    Ensures you have the latest chart versions available.

    Search for Available Charts: helm search repo bitnami/mongodb

    Find the chart you need and check its available versions.

    Research Default Values:

        Examine the default values.yaml file in the chart's documentation.

        Understand which values you need to modify for your use case.

    Create Custom Values File:

        Create a file, e.g., my-mongodb-values.yaml, with your custom settings.

        Only include values that differ from the defaults.

    Install the Chart with Custom Values: helm install my-mongodb bitnami/mongodb -f my-mongodb-values.yaml

    This command installs MongoDB, applying your custom configuration on top of the default values. Only the fields specified in my-mongodb-values.yaml will override the corresponding default values, while all other settings remain at their default.

Research Before Deployment

Thoroughly read the chart's documentation, and understand the implications of changing default values.
Conclusion

By leveraging Helm as a package manager, you can significantly simplify the deployment and management of complex software in Kubernetes environments, allowing you to focus more on your application and less on the intricacies of Kubernetes configurations.